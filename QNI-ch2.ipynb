{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f7ad5-fd1b-478d-a0d8-5371f13909fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "added perturbation for features, earlier it was only for centroids\n",
    "formula for epsilon\n",
    "\n",
    "contains base model code, additional model training code, classification report, after-attack accuracy\n",
    "\n",
    "model saved as: best_QNI_model_2\n",
    "accuracy = 96%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc27b8d9-94d1-418b-b075-e5ad80084252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**dataset loaded**\n",
      "Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a778154bcfe441e7ad78b54aca2146d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 301\u001b[0m\n\u001b[1;32m    298\u001b[0m loss_clean \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(clean_logits, y)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Perturbed path (gradient-based on features)\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m feats_pert \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_noise_on_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m feats_pert_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(feats_pert)\n\u001b[1;32m    303\u001b[0m q_out_pert \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([model\u001b[38;5;241m.\u001b[39mq_layer(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feats_pert_t])\n",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m, in \u001b[0;36mgradient_noise_on_features\u001b[0;34m(model, x, y, epsilon)\u001b[0m\n\u001b[1;32m    199\u001b[0m logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mclassifier(q_out)\n\u001b[1;32m    201\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[0;32m--> 202\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Get gradient w.r.t. input features\u001b[39;00m\n\u001b[1;32m    205\u001b[0m feats_grad \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from matplotlib import pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane.qnn import TorchLayer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#for loss function \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "# ========== DEVICE ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== PARAMETERS ==========\n",
    "n_qubits = 6\n",
    "batch_size = 16\n",
    "num_classes = 25\n",
    "num_epochs = 50\n",
    "lr = 0.0005\n",
    "\n",
    "# ========== TRANSFORMS WITH DATA AUGMENTATION ==========\n",
    "# ✅ For training (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# ✅ For validation and test (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "# ========== DATASETS ==========\n",
    "train_dataset = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/train', transform=train_transform)\n",
    "val_dataset   = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/val', transform=eval_transform)\n",
    "test_dataset  = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/test', transform=eval_transform)\n",
    "print(\"**dataset loaded**\")\n",
    "# ========== CLASS WEIGHTS ==========\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "labels = [label for _, label in train_dataset.samples]\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(labels),\n",
    "                                     y=labels)\n",
    "class_wts = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# ========== QUANTUM CIRCUIT ==========\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "\n",
    "def quantum_circuit(inputs, weights):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "    for l in range(weights.shape[0]):\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(weights[l][i], wires=i)\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "    \n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "weight_shapes = {\"weights\": (6, n_qubits)}\n",
    "\n",
    "\n",
    "# ========== CNN + QNN MODEL ==========\n",
    "class FeatureReduce(nn.Module):\n",
    "    def __init__(self, final_dim, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),    # 128 -> 64\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),   # 64 -> 32\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 32 -> 16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 16 -> 8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # ⬅️ Extra block: 8 -> 4\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                # 4×4 -> 1×1\n",
    "        )\n",
    "        self.fc = nn.Linear(128, final_dim)  # ⬅️ Changed from 64 to 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class HybridQNN(nn.Module):\n",
    "    def __init__(self, n_qubits, num_classes):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = FeatureReduce(final_dim=n_qubits)\n",
    "        self.q_layer = TorchLayer(quantum_circuit, weight_shapes)\n",
    "\n",
    "        # Adding 4-layer MLP after quantum layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_qubits, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.tanh(x)\n",
    "        q_out = torch.stack([self.q_layer(f) for f in x])\n",
    "        return self.classifier(q_out)\n",
    "\n",
    "# ========== TRAINING ==========\n",
    "print(\"Starting training\")\n",
    "\n",
    "# Applying gradient based pertubations of features\n",
    "def gradient_noise_on_features(model, x, y, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Compute gradient of loss w.r.t. extracted features and perturb them.\n",
    "    Returns: perturbed feature tensor [B, n_qubits]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward: extract features, apply tanh, quantum, classify\n",
    "    feats = model.feature_extractor(x)  # pre-tanh features [B, n_qubits]\n",
    "    feats_tanh = torch.tanh(feats)\n",
    "    \n",
    "    q_out = torch.stack([model.q_layer(f) for f in feats_tanh])\n",
    "    logits = model.classifier(q_out)\n",
    "    \n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradient w.r.t. input features\n",
    "    feats_grad = x.grad.data\n",
    "    x_pert = x + epsilon * feats_grad.sign()\n",
    "    x_pert = torch.clamp(x_pert, -1, 1)\n",
    "    \n",
    "    # Re-extract features from perturbed image\n",
    "    feats_pert = model.feature_extractor(x_pert)\n",
    "    \n",
    "    return feats_pert.detach()\n",
    "\n",
    "# ── 2) Precompute class‐centroids in feature space ──────────────────────────\n",
    "def compute_centroids(model, loader, device, num_classes):\n",
    "    model.eval()\n",
    "    sums = torch.zeros(num_classes, n_qubits, device=device)\n",
    "    counts = torch.zeros(num_classes, device=device)\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            feats = model.feature_extractor(x)      # pre‐tanh features\n",
    "            for c in range(num_classes):\n",
    "                mask = (y==c)\n",
    "                if mask.any():\n",
    "                    sums[c] += feats[mask].sum(0)\n",
    "                    counts[c] += mask.sum()\n",
    "    return sums / counts.unsqueeze(1)\n",
    "\n",
    "# ── 3) QNI perturbation function ────────────────────────────────────────────\n",
    "def gradient_based_noise(model, x, y, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    x: input image batch [B, C, H, W]\n",
    "    y: labels\n",
    "    \"\"\"\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    model.eval()\n",
    "\n",
    "    # Get output logits\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    # Compute gradient of loss w.r.t input\n",
    "    loss.backward()\n",
    "    grad = x.grad.data  # [B, C, H, W]\n",
    "\n",
    "    # Normalize gradient and perturb input\n",
    "    grad_sign = grad.sign()\n",
    "    x_pert = x + epsilon * grad_sign\n",
    "    x_pert = torch.clamp(x_pert, -1, 1)  # Keep within normalized bounds\n",
    "\n",
    "    return x_pert.detach()\n",
    "\n",
    "# … everything above stays the same up to compute_centroids …\n",
    "\n",
    "# ── 4) Training loop with QNI ──────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HybridQNN(n_qubits, num_classes).to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=5)\n",
    "\n",
    "# Initialize best validation accuracy\n",
    "best_val_acc = 0.0\n",
    "best_model_path = \"best_QNI_model_2.pth\"\n",
    "\n",
    "\n",
    "# helper to evaluate on a loader\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total   += y.size(0)\n",
    "    return correct/total\n",
    "\n",
    "# initial centroids before training\n",
    "centroids = compute_centroids(model, train_loader, device, num_classes)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    # every 5 epochs, recompute centroids on the *current* model:\n",
    "    if epoch % 5 == 0:\n",
    "        centroids = compute_centroids(model, train_loader, device, num_classes)\n",
    "    \n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0, 0, 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch} [train]\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        ### changed\n",
    "        # Clean path\n",
    "        feats = model.feature_extractor(x)\n",
    "        clean_logits = model(x)\n",
    "        loss_clean = F.cross_entropy(clean_logits, y)\n",
    "\n",
    "        # Perturbed path (gradient-based on features)\n",
    "        feats_pert = gradient_noise_on_features(model, x, y, epsilon=0.1)\n",
    "        feats_pert_t = torch.tanh(feats_pert)\n",
    "        q_out_pert = torch.stack([model.q_layer(f) for f in feats_pert_t])\n",
    "        pert_logits = model.classifier(q_out_pert)\n",
    "        loss_pert = F.cross_entropy(pert_logits, y)\n",
    "\n",
    "        # Joint loss\n",
    "        loss = 0.8 * loss_clean + 0.2 * loss_pert\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "        # track\n",
    "        running_loss   += loss.item() * x.size(0)\n",
    "        running_correct += (clean_logits.argmax(1) == y).sum().item()\n",
    "        running_total   += y.size(0)\n",
    "\n",
    "    # step scheduler on *average* training loss\n",
    "    avg_train_loss = running_loss / running_total\n",
    "    sched.step(avg_train_loss)\n",
    "\n",
    "    train_acc = running_correct / running_total\n",
    "    val_acc   = evaluate(model, val_loader)\n",
    "    print(f\"\\nEpoch {epoch:2d} — train loss: {avg_train_loss:.4f}, \"\n",
    "      f\"train acc: {train_acc:.4f}, val acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(), \n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'val_accuracy': val_acc\n",
    "        }, best_model_path)F\n",
    "        print(f\"✅ Best model saved at epoch {epoch} with val_acc: {val_acc:.4f}\\n\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15bc4492-29c8-4ac7-9104-33d61774282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Accuracy: 0.9697\n",
      "✅ Test Accuracy: 0.9594\n"
     ]
    }
   ],
   "source": [
    "## to evaluate the saved model\n",
    "\n",
    "# === Evaluate saved model ===\n",
    "model = HybridQNN(n_qubits, num_classes).to(device)\n",
    "checkpoint = torch.load(\"best_QNI_model_2.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "val_acc = evaluate(model, val_loader)\n",
    "test_acc = evaluate(model, test_loader)\n",
    "\n",
    "print(f\"✅ Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"✅ Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d0c1f5-8b96-4ab0-8f09-00d4f4da7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded. Resuming from epoch 37 with best val_acc = 0.9697\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feats_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Resume training for 10 more epochs\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 🔁 Re-initialize training data for retraining\u001b[39;00m\n\u001b[1;32m     17\u001b[0m num_adv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Number of adversarial samples to generate (tuneable)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m feats_adv \u001b[38;5;241m=\u001b[39m \u001b[43mfeats_train\u001b[49m[:num_adv]\n\u001b[1;32m     19\u001b[0m labels_adv \u001b[38;5;241m=\u001b[39m labels_train[:num_adv]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Generate PGD adversarial examples\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feats_train' is not defined"
     ]
    }
   ],
   "source": [
    "## train the model again\n",
    "\n",
    "# === Load saved model for continued training ===\n",
    "model = HybridQNN(n_qubits, num_classes).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"best_QNI_model_2.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_val_acc = checkpoint['val_accuracy']\n",
    "print(f\"✅ Model loaded. Resuming from epoch {start_epoch} with best val_acc = {best_val_acc:.4f}\")\n",
    "\n",
    "# Resume training for 10 more epochs\n",
    "for epoch in range(start_epoch, start_epoch + 5):\n",
    "    if epoch % 5 == 0:\n",
    "        centroids = compute_centroids(model, train_loader, device, num_classes)\n",
    "\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0, 0, 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch} [train]\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Clean path\n",
    "        feats = model.feature_extractor(x)\n",
    "        clean_logits = model(x)\n",
    "        loss_clean = F.cross_entropy(clean_logits, y)\n",
    "\n",
    "        # Perturbed path using gradient-based perturbation on features\n",
    "        feats_pert = gradient_noise_on_features(model, x, y, epsilon=0.1)\n",
    "        feats_pert_t = torch.tanh(feats_pert)\n",
    "        q_out_pert = torch.stack([model.q_layer(f) for f in feats_pert_t])\n",
    "        pert_logits = model.classifier(q_out_pert)\n",
    "        loss_pert = F.cross_entropy(pert_logits, y)\n",
    "\n",
    "        # Joint loss and backprop\n",
    "        loss = 0.8 * loss_clean + 0.2 * loss_pert\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Track stats\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        running_correct += (clean_logits.argmax(1) == y).sum().item()\n",
    "        running_total += y.size(0)\n",
    "\n",
    "    # Scheduler step\n",
    "    avg_train_loss = running_loss / running_total\n",
    "    sched.step(avg_train_loss)\n",
    "\n",
    "    train_acc = running_correct / running_total\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"\\nEpoch {epoch:2d} — train loss: {avg_train_loss:.4f}, \"\n",
    "          f\"train acc: {train_acc:.4f}, val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model if improved\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'val_accuracy': val_acc\n",
    "        }, \"best_QNI_model_2.pth\")\n",
    "        print(f\"✅ Best model updated at epoch {epoch} with val_acc: {val_acc:.4f}\\n\")\n",
    "    else:\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc13cec6-9124-4401-987b-fd1c0e4bfb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     Adialer.C       1.00      1.00      1.00        14\n",
      "     Agent.FYI       1.00      1.00      1.00        13\n",
      "     Allaple.A       1.00      0.99      0.99       296\n",
      "     Allaple.L       1.00      1.00      1.00       160\n",
      " Alueron.gen!J       1.00      0.95      0.98        21\n",
      "     Autorun.K       0.00      0.00      0.00        12\n",
      "       C2LOP.P       0.72      0.81      0.76        16\n",
      "   C2LOP.gen!g       0.78      0.70      0.74        20\n",
      "Dialplatform.B       1.00      1.00      1.00        20\n",
      "     Dontovo.A       1.00      1.00      1.00        17\n",
      "      Fakerean       1.00      0.97      0.99        39\n",
      " Instantaccess       0.92      1.00      0.96        44\n",
      "    Lolyda.AA1       1.00      0.95      0.98        22\n",
      "    Lolyda.AA2       1.00      1.00      1.00        21\n",
      "    Lolyda.AA3       1.00      1.00      1.00        13\n",
      "     Lolyda.AT       1.00      1.00      1.00        17\n",
      "   Malex.gen!J       0.94      1.00      0.97        15\n",
      " Obfuscator.AD       1.00      1.00      1.00        15\n",
      "      Rbot!gen       1.00      1.00      1.00        17\n",
      "    Skintrim.N       1.00      1.00      1.00         8\n",
      " Swizzor.gen!E       0.52      0.93      0.67        14\n",
      " Swizzor.gen!I       1.00      0.14      0.25        14\n",
      "         VB.AT       1.00      1.00      1.00        42\n",
      "    Wintrim.BX       1.00      1.00      1.00        11\n",
      "       Yuner.A       0.87      1.00      0.93        80\n",
      "\n",
      "      accuracy                           0.96       961\n",
      "     macro avg       0.91      0.90      0.89       961\n",
      "  weighted avg       0.95      0.96      0.95       961\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/netsec1/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/netsec1/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/netsec1/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# Initialize the model\n",
    "model = HybridQNN(n_qubits=n_qubits, num_classes=num_classes).to(device)\n",
    "\n",
    "# ✅ Load only the model weights from the saved checkpoint\n",
    "checkpoint = torch.load(\"best_QNI_model_2.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])  # ⬅️ fix\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Get class names from the test dataset\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e593f34-746b-4998-9d87-d575d774a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Clean Test Accuracy: 95.94%\n",
      "FGSM Adversarial Accuracy: 41.73%\n",
      "PGD Adversarial Accuracy: 25.91%\n"
     ]
    }
   ],
   "source": [
    "## attacking the model with FGSM and PGD\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ===== FGSM Attack =====\n",
    "def fgsm_attack(model, x, y, epsilon=0.1, device='cuda'):\n",
    "    model.eval()\n",
    "    x_adv = x.clone().detach().to(device).requires_grad_(True)\n",
    "    y = y.to(device)\n",
    "\n",
    "    logits = model(x_adv)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    x_adv = x_adv + epsilon * x_adv.grad.sign()\n",
    "    x_adv = torch.clamp(x_adv, min=-1.0, max=1.0)\n",
    "    return x_adv.detach()\n",
    "\n",
    "# ===== PGD Attack =====\n",
    "def pgd_attack(model, x, y, eps=0.1, alpha=0.02, iters=10, device='cuda'):\n",
    "    model.eval()\n",
    "    x_orig = x.clone().detach().to(device)\n",
    "    x_adv  = x_orig + torch.empty_like(x_orig).uniform_(-eps, eps)\n",
    "    x_adv  = torch.clamp(x_adv, -1.0, 1.0).detach()\n",
    "\n",
    "    y = y.to(device)\n",
    "    for _ in range(iters):\n",
    "        x_adv.requires_grad_(True)\n",
    "        logits = model(x_adv)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        x_adv = x_adv + alpha * x_adv.grad.sign()\n",
    "        delta = torch.clamp(x_adv - x_orig, min=-eps, max=eps)\n",
    "        x_adv = torch.clamp(x_orig + delta, -1.0, 1.0).detach()\n",
    "    return x_adv\n",
    "\n",
    "# ===== Evaluate Attack =====\n",
    "def test_adversarial(model, loader, attack_fn, attack_name, **attack_kwargs):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_adv = attack_fn(model, x, y, **attack_kwargs)\n",
    "        logits = model(x_adv)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"{attack_name} Adversarial Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# ===== Load Model from best_QNI_model_2.pth =====\n",
    "model = HybridQNN(n_qubits=n_qubits, num_classes=num_classes).to(device)\n",
    "checkpoint = torch.load(\"best_QNI_model_2.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# ===== Run Evaluations =====\n",
    "# Clean accuracy\n",
    "clean_acc = evaluate(model, test_loader)\n",
    "print(f\"\\n✅ Clean Test Accuracy: {clean_acc*100:.2f}%\")\n",
    "\n",
    "# FGSM attack\n",
    "test_adversarial(\n",
    "    model, test_loader,\n",
    "    attack_fn=fgsm_attack,\n",
    "    attack_name=\"FGSM\",\n",
    "    epsilon=0.1,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# PGD attack\n",
    "test_adversarial(\n",
    "    model, test_loader,\n",
    "    attack_fn=pgd_attack,\n",
    "    attack_name=\"PGD\",\n",
    "    eps=0.1,\n",
    "    alpha=0.02,\n",
    "    iters=10,\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
