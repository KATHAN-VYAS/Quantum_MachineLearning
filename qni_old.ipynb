{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5bb03e-5c8e-4880-8d55-d64b9cd0d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np \n",
    "import random \n",
    "import os \n",
    "from torchvision.datasets import ImageFolder\n",
    "import pennylane as qml \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm.notebook import tqdm \n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# %%\n",
    "class Focal_loss(nn.Module):\n",
    "    def __init__(self, alpha, gamma, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        CE = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-CE)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * CE\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "# %%\n",
    "class QNI_CCP:\n",
    "    \"\"\"\n",
    "    Quantum-aware Noise Injection with Class-Conditional Perturbation\n",
    "    Enhanced with adaptive perturbation and better regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits, n_layers, num_classes, epsilon_base=0.05, \n",
    "                 update_freq=100, alpha=0.9, adaptive_epsilon=True):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon_base = epsilon_base\n",
    "        self.update_freq = update_freq\n",
    "        self.alpha = alpha\n",
    "        self.adaptive_epsilon = adaptive_epsilon\n",
    "        \n",
    "        # Class-conditional statistics\n",
    "        self.class_means = {}\n",
    "        self.class_counts = defaultdict(int)\n",
    "        self.update_counter = 0\n",
    "        \n",
    "        # Quantum complexity metrics\n",
    "        self.circuit_depth = n_layers\n",
    "        self.entangling_gates = (n_qubits - 1) * n_layers\n",
    "        self.quantum_complexity = self._compute_quantum_complexity()\n",
    "        \n",
    "        # Adaptive perturbation tracking\n",
    "        self.current_epsilon = epsilon_base\n",
    "        self.performance_history = []\n",
    "        \n",
    "    def _compute_quantum_complexity(self):\n",
    "        \"\"\"Compute quantum-aware scaling factor based on circuit properties\"\"\"\n",
    "        depth_factor = min(self.circuit_depth / 10.0, 1.0)\n",
    "        entanglement_factor = min(self.entangling_gates / (self.n_qubits * 5), 1.0)\n",
    "        \n",
    "        complexity = 0.5 * depth_factor + 0.5 * entanglement_factor\n",
    "        epsilon_q = 1.0 - complexity\n",
    "        return max(epsilon_q, 0.1)\n",
    "    \n",
    "    def update_epsilon(self, train_acc, val_acc):\n",
    "        \"\"\"Adaptively adjust epsilon based on overfitting detection\"\"\"\n",
    "        if len(self.performance_history) > 0:\n",
    "            overfitting_gap = train_acc - val_acc\n",
    "            \n",
    "            if overfitting_gap > 0.15:  # Significant overfitting\n",
    "                self.current_epsilon = min(self.current_epsilon * 1.2, 0.3)\n",
    "            elif overfitting_gap < 0.05:  # Underfitting or good fit\n",
    "                self.current_epsilon = max(self.current_epsilon * 0.9, 0.01)\n",
    "        \n",
    "        self.performance_history.append((train_acc, val_acc))\n",
    "        if len(self.performance_history) > 10:\n",
    "            self.performance_history.pop(0)\n",
    "    \n",
    "    def update_class_statistics(self, features, labels):\n",
    "        \"\"\"Update running class means using exponential moving average\"\"\"\n",
    "        features = features.detach().cpu()\n",
    "        labels = labels.detach().cpu()\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = label.item()\n",
    "            feature_vec = features[i]\n",
    "            \n",
    "            if label_int not in self.class_means:\n",
    "                self.class_means[label_int] = feature_vec.clone()\n",
    "            else:\n",
    "                self.class_means[label_int] = (self.alpha * self.class_means[label_int] + \n",
    "                                             (1 - self.alpha) * feature_vec)\n",
    "            \n",
    "            self.class_counts[label_int] += 1\n",
    "        \n",
    "        self.update_counter += 1\n",
    "    \n",
    "    def compute_gradient_sensitivity(self, model, features, labels, loss_fn):\n",
    "        \"\"\"Compute gradient sensitivity with better numerical stability\"\"\"\n",
    "        device = features.device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            extracted_features = model.featureextractor(features)\n",
    "        \n",
    "        extracted_features.requires_grad_(True)\n",
    "        \n",
    "        q_outputs = []\n",
    "        for i in range(extracted_features.size(0)):\n",
    "            q_out = model.q_layer(torch.tanh(extracted_features[i]))\n",
    "            q_outputs.append(q_out)\n",
    "        q_out_batch = torch.stack(q_outputs)\n",
    "        \n",
    "        outputs = model.classifier(q_out_batch)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        gradients = torch.autograd.grad(loss, extracted_features,\n",
    "                                        retain_graph=True, create_graph=True)[0]\n",
    "        \n",
    "        # Add small epsilon for numerical stability\n",
    "        sensitivity = torch.abs(gradients).detach() + 1e-8\n",
    "        \n",
    "        return sensitivity\n",
    "\n",
    "    def generate_perturbation(self, features, labels, model, loss_fn, training=True):\n",
    "        \"\"\"Generate QNI-CCP perturbations with improved stability\"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            extracted_features = model.featureextractor(features)\n",
    "        \n",
    "        if len(self.class_means) < 2:\n",
    "            return torch.tanh(extracted_features)\n",
    "        \n",
    "        sensitivity = self.compute_gradient_sensitivity(model, features, labels, loss_fn)\n",
    "        perturbed_features = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            current_label = labels[i].item()\n",
    "            current_feature = extracted_features[i]\n",
    "            current_sensitivity = sensitivity[i]\n",
    "            \n",
    "            available_classes = [c for c in self.class_means\n",
    "                                 if c != current_label and self.class_counts[c] > 5]\n",
    "            if not available_classes:\n",
    "                perturbed_features.append(torch.tanh(current_feature))\n",
    "                continue\n",
    "            \n",
    "            target_class = random.choice(available_classes)\n",
    "            target_mean = self.class_means[target_class].to(device)\n",
    "            \n",
    "            direction = target_mean - current_feature\n",
    "            direction = direction / (torch.norm(direction) + 1e-8)  # Normalize\n",
    "            \n",
    "            # Use adaptive epsilon\n",
    "            epsilon_q = self.quantum_complexity\n",
    "            current_eps = self.current_epsilon if self.adaptive_epsilon else self.epsilon_base\n",
    "            \n",
    "            # Scale perturbation more conservatively\n",
    "            perturbation_scale = current_eps * epsilon_q * 0.1\n",
    "            scaled_perturbation = perturbation_scale * current_sensitivity * direction\n",
    "            \n",
    "            perturbed_feature = current_feature + scaled_perturbation\n",
    "            perturbed_features.append(torch.tanh(perturbed_feature))\n",
    "        \n",
    "        return torch.stack(perturbed_features)\n",
    "    \n",
    "    def apply_qni_ccp(self, model, features, labels, loss_fn, training=True, \n",
    "                     mix_ratio=0.3, epoch=0):\n",
    "        \"\"\"Apply QNI-CCP with progressive mixing\"\"\"\n",
    "        # Get clean features and outputs\n",
    "        clean_features = model.featureextractor(features)\n",
    "        clean_features = torch.tanh(clean_features)\n",
    "        clean_q_outputs = []\n",
    "        for i in range(clean_features.size(0)):\n",
    "            q_out = model.q_layer(clean_features[i])\n",
    "            clean_q_outputs.append(q_out)\n",
    "        clean_q_out_batch = torch.stack(clean_q_outputs)\n",
    "        clean_outputs = model.classifier(clean_q_out_batch)\n",
    "        \n",
    "        if training:\n",
    "            self.update_class_statistics(clean_features, labels)\n",
    "        \n",
    "        # Progressive mixing: start with lower ratio, increase over time\n",
    "        if training:\n",
    "            progressive_ratio = min(mix_ratio * (1 + epoch * 0.02), 0.6)\n",
    "        else:\n",
    "            progressive_ratio = mix_ratio\n",
    "            \n",
    "        # Apply QNI-CCP with probability based on progressive ratio\n",
    "        if (training and len(self.class_means) >= 2 and \n",
    "            random.random() < progressive_ratio):\n",
    "            \n",
    "            perturbed_features = self.generate_perturbation(features, labels, \n",
    "                                                          model, loss_fn, training)\n",
    "            \n",
    "            perturbed_q_outputs = []\n",
    "            for i in range(perturbed_features.size(0)):\n",
    "                q_out = model.q_layer(perturbed_features[i])\n",
    "                perturbed_q_outputs.append(q_out)\n",
    "            perturbed_q_out_batch = torch.stack(perturbed_q_outputs)\n",
    "            perturbed_outputs = model.classifier(perturbed_q_out_batch)\n",
    "            \n",
    "            clean_loss = loss_fn(clean_outputs, labels)\n",
    "            perturbed_loss = loss_fn(perturbed_outputs, labels)\n",
    "            \n",
    "            # Adaptive loss mixing based on performance\n",
    "            clean_weight = 0.9 if epoch < 10 else 0.7\n",
    "            perturbed_weight = 1.0 - clean_weight\n",
    "            \n",
    "            total_loss = clean_weight * clean_loss + perturbed_weight * perturbed_loss\n",
    "            \n",
    "            return total_loss, clean_outputs\n",
    "        else:\n",
    "            loss = loss_fn(clean_outputs, labels)\n",
    "            return loss, clean_outputs\n",
    "\n",
    "# %%\n",
    "# Enhanced model with better regularization\n",
    "class FeatureReduce(nn.Module):\n",
    "    def __init__(self, final_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout * 0.5),  # Spatial dropout\n",
    "\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout * 0.6),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout * 0.7),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout * 0.8),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, final_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(final_dim * 2, final_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class HybridQnn(nn.Module):\n",
    "    def __init__(self, n_qubits, num_classes, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.featureextractor = FeatureReduce(final_dim=n_qubits, dropout=dropout)\n",
    "        self.q_layer = qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)\n",
    "        \n",
    "        # Simplified classifier to reduce overfitting\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_qubits, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.8),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.featureextractor(x)\n",
    "        x = torch.tanh(x)\n",
    "        # q_outputs = []\n",
    "        for i in range(x.size(0)):\n",
    "            q_out = self.q_layer(x[i])\n",
    "            q_outputs.append(q_out)\n",
    "        q_out_batch = torch.stack(q_outputs)\n",
    "        # q_out_batch=self.q_layer(x)\n",
    "        return self.classifier(q_out_batch)\n",
    "\n",
    "# %%\n",
    "# Enhanced training transforms with more augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    # Add random noise\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.02)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# %%\n",
    "# Enhanced training function with better monitoring\n",
    "def train_with_qni_ccp(model, data_loader, loss_fn, optimizer, device, qni_ccp, epoch=0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply QNI-CCP with epoch information\n",
    "        loss, outputs = qni_ccp.apply_qni_ccp(model, inputs, labels, loss_fn, \n",
    "                                             training=True, mix_ratio=0.3, epoch=epoch)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping with adaptive norm\n",
    "        max_norm = 1.0 if epoch < 10 else 0.5\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(data_loader), correct / len(data_loader.dataset)\n",
    "\n",
    "# %%\n",
    "# Enhanced evaluation function\n",
    "def evaluate_with_qni_ccp(model, dataloader, loss_fn, device, qni_ccp, test_robustness=False):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    robust_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if test_robustness:\n",
    "                _, perturbed_outputs = qni_ccp.apply_qni_ccp(model, inputs, labels, \n",
    "                                                           loss_fn, training=False, \n",
    "                                                           mix_ratio=1.0)\n",
    "                _, robust_predicted = torch.max(perturbed_outputs.data, 1)\n",
    "                robust_correct += (robust_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    robust_accuracy = robust_correct / total if test_robustness else None\n",
    "    \n",
    "    return total_loss / len(dataloader), accuracy, robust_accuracy\n",
    "\n",
    "# %%\n",
    "# Enhanced hyperparameters for better generalization\n",
    "def get_enhanced_config():\n",
    "    return {\n",
    "        'n_qubits': 6,\n",
    "        'n_layers': 2,\n",
    "        'num_classes': 25,\n",
    "        'batch_size': 32,  # Increased batch size\n",
    "        'num_epochs': 100,\n",
    "        'lr': 0.001,      # Reduced learning rate\n",
    "        'weight_decay': 5e-4,  # Increased weight decay\n",
    "        'dropout': 0.5,\n",
    "        'early_stopping_patience': 10,\n",
    "        'lr_scheduler_patience': 5,\n",
    "        'lr_scheduler_factor': 0.7,\n",
    "        'qni_ccp_epsilon': 0.05,  # Reduced epsilon\n",
    "        'warmup_epochs': 8\n",
    "    }\n",
    "\n",
    "config = get_enhanced_config()\n",
    "\n",
    "# %%\n",
    "# Enhanced learning rate scheduler\n",
    "class CosineAnnealingWarmupRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1.0):\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.gamma = gamma\n",
    "        self.T_cur = 0\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr) * self.T_cur / self.T_up + base_lr\n",
    "                    for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * \n",
    "                    (1 + np.cos(np.pi * (self.T_cur - self.T_up) / (self.T_0 - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.T_cur = epoch\n",
    "        super().step(epoch)\n",
    "\n",
    "# %%\n",
    "# Enhanced training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "# %%\n",
    "# Initialize model and training components\n",
    "model = HybridQnn(n_qubits=config['n_qubits'], \n",
    "                  num_classes=config['num_classes'], \n",
    "                  dropout=config['dropout'])\n",
    "model.to(device)\n",
    "\n",
    "# Enhanced optimizer with different learning rates for different parts\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.featureextractor.parameters(), 'lr': config['lr']},\n",
    "    {'params': model.q_layer.parameters(), 'lr': config['lr'] * 0.5},  # Lower LR for quantum part\n",
    "    {'params': model.classifier.parameters(), 'lr': config['lr']}\n",
    "], weight_decay=config['weight_decay'])\n",
    "\n",
    "# Enhanced scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=config['lr_scheduler_factor'], \n",
    "    patience=config['lr_scheduler_patience']\n",
    ")\n",
    "\n",
    "# Initialize enhanced QNI-CCP\n",
    "qni_ccp = QNI_CCP(n_qubits=config['n_qubits'], \n",
    "                  n_layers=config['n_layers'], \n",
    "                  num_classes=config['num_classes'],\n",
    "                  epsilon_base=config['qni_ccp_epsilon'], \n",
    "                  update_freq=100, \n",
    "                  alpha=0.9,\n",
    "                  adaptive_epsilon=True)\n",
    "\n",
    "# %%\n",
    "# Enhanced training loop with better monitoring\n",
    "def enhanced_training_loop(model, train_loader, val_loader, test_loader, \n",
    "                          loss_fn, optimizer, scheduler, qni_ccp, config, device):\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    robust_accs = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    print(\"Starting enhanced training with adaptive QNI-CCP...\")\n",
    "    print(f\"Configuration: {config}\")\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # Training\n",
    "        train_loss, train_acc = train_with_qni_ccp(model, train_loader, loss_fn, \n",
    "                                                  optimizer, device, qni_ccp, epoch)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc, robust_acc = evaluate_with_qni_ccp(model, val_loader, loss_fn, \n",
    "                                                              device, qni_ccp, \n",
    "                                                              test_robustness=True)\n",
    "        \n",
    "        # Update adaptive epsilon in QNI-CCP\n",
    "        qni_ccp.update_epsilon(train_acc, val_acc)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        robust_accs.append(robust_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Enhanced logging\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Robust Acc: {robust_acc:.4f}\")\n",
    "        print(f\"Overfitting Gap: {(train_acc - val_acc):.4f}\")\n",
    "        print(f\"Current QNI-CCP ε: {qni_ccp.current_epsilon:.4f}\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Enhanced early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'qni_ccp_state': {\n",
    "                    'class_means': qni_ccp.class_means,\n",
    "                    'class_counts': qni_ccp.class_counts,\n",
    "                    'quantum_complexity': qni_ccp.quantum_complexity,\n",
    "                    'current_epsilon': qni_ccp.current_epsilon\n",
    "                },\n",
    "                'epoch': epoch,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'config': config\n",
    "            }, \"best_model_enhanced_qni_ccp.pth\")\n",
    "            print(\"💾 Best model saved.\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= config['early_stopping_patience']:\n",
    "            print(f\"⏹️ Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs, robust_accs\n",
    "\n",
    "# %%\n",
    "print(\"Enhanced QNI-CCP training implementation ready!\")\n",
    "print(\"Key improvements:\")\n",
    "print(\"1. Adaptive perturbation strength based on overfitting detection\")\n",
    "print(\"2. Enhanced regularization with spatial dropout\")\n",
    "print(\"3. Progressive perturbation mixing\")\n",
    "print(\"4. Better gradient clipping and learning rate scheduling\")\n",
    "print(\"5. Improved numerical stability\")\n",
    "print(\"6. Enhanced data augmentation\")\n",
    "print(\"7. Different learning rates for different model components\")\n",
    "print(\"8. Better early stopping and model checkpointing\")\n",
    "\n",
    "# %%\n",
    "# Define quantum circuit and weight shapes (needed for the model)\n",
    "dev = qml.device(\"lightning.qubit\", wires=config['n_qubits'], shots=None)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def circuit(inputs, weights):\n",
    "    # Initial encoding\n",
    "    for i in range(config['n_qubits']):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "    # Variational layers with improved entanglement\n",
    "    for l in range(weights.shape[0]):\n",
    "        # Parameterized rotations\n",
    "        for i in range(config['n_qubits']):\n",
    "            qml.RY(weights[l][i][0], wires=i)\n",
    "            qml.RZ(weights[l][i][1], wires=i)\n",
    "        \n",
    "        # Circular entanglement for better connectivity\n",
    "        for i in range(config['n_qubits']):\n",
    "            qml.CNOT(wires=[i, (i + 1) % config['n_qubits']])\n",
    "        \n",
    "        # Data re-encoding (every other layer)\n",
    "        if l % 2 == 1:\n",
    "            for i in range(config['n_qubits']):\n",
    "                qml.RY(inputs[i] * 0.1, wires=i)\n",
    "    \n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(config['n_qubits'])]\n",
    "\n",
    "weight_shapes = {\"weights\": (config['n_layers'], config['n_qubits'], 2)}\n",
    "\n",
    "# %%\n",
    "# COMPLETE EXECUTION BLOCK - Run this to train the model\n",
    "print(\"=\"*80)\n",
    "print(\"🚀 STARTING ENHANCED QNI-CCP TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load datasets (update paths as needed)\n",
    "train_dataset = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/val', transform=test_transform)\n",
    "test_dataset = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/test', transform=test_transform)\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
    "                         shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
    "                       shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], \n",
    "                        shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Initialize loss function with class weights\n",
    "labels = [label for _, label in train_dataset.samples]\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "loss_fn = Focal_loss(alpha=1, gamma=2)\n",
    "\n",
    "print(f\"Loss function initialized with {len(class_weights)} classes\")\n",
    "\n",
    "# %%\n",
    "# Run the enhanced training\n",
    "print(\"Starting enhanced training loop...\")\n",
    "train_losses, val_losses, train_accs, val_accs, robust_accs = enhanced_training_loop(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    loss_fn, optimizer, scheduler, qni_ccp, config, device\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Training completion and visualization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📈 TRAINING COMPLETED - GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot comprehensive training results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0, 1].plot(train_accs, label='Train Acc', linewidth=2)\n",
    "axes[0, 1].plot(val_accs, label='Val Acc', linewidth=2)\n",
    "axes[0, 1].set_title('Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Robustness analysis\n",
    "axes[0, 2].plot(val_accs, label='Standard Acc', linewidth=2)\n",
    "axes[0, 2].plot(robust_accs, label='Robust Acc', linewidth=2)\n",
    "axes[0, 2].set_title('Robustness Analysis', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Accuracy')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting gap\n",
    "overfitting_gap = [train_accs[i] - val_accs[i] for i in range(len(train_accs))]\n",
    "axes[1, 0].plot(overfitting_gap, label='Overfitting Gap', linewidth=2, color='red')\n",
    "axes[1, 0].axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='Warning Threshold')\n",
    "axes[1, 0].axhline(y=0.15, color='red', linestyle='--', alpha=0.7, label='Critical Threshold')\n",
    "axes[1, 0].set_title('Overfitting Monitoring', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Train Acc - Val Acc')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate evolution (if available)\n",
    "if hasattr(scheduler, 'get_last_lr'):\n",
    "    lr_history = [scheduler.get_last_lr()[0] for _ in range(len(train_losses))]\n",
    "    axes[1, 1].plot(lr_history, label='Learning Rate', linewidth=2, color='green')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'LR History\\nNot Available', \n",
    "                   horizontalalignment='center', verticalalignment='center',\n",
    "                   transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "\n",
    "# QNI-CCP epsilon evolution\n",
    "if hasattr(qni_ccp, 'performance_history') and qni_ccp.performance_history:\n",
    "    epsilon_history = [qni_ccp.current_epsilon] * len(train_losses)  # Simplified\n",
    "    axes[1, 2].plot(epsilon_history, label='QNI-CCP ε', linewidth=2, color='purple')\n",
    "    axes[1, 2].set_title('Adaptive Perturbation Strength', fontsize=14, fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Epsilon')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, f'Final ε: {qni_ccp.current_epsilon:.4f}', \n",
    "                   horizontalalignment='center', verticalalignment='center',\n",
    "                   transform=axes[1, 2].transAxes, fontsize=12)\n",
    "    axes[1, 2].set_title('Adaptive Perturbation Strength', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Load best model and final evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏆 FINAL EVALUATION WITH BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "if os.path.exists(\"best_model_enhanced_qni_ccp.pth\"):\n",
    "    checkpoint = torch.load(\"best_model_enhanced_qni_ccp.pth\", weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Restore QNI-CCP state\n",
    "    qni_ccp.class_means = checkpoint['qni_ccp_state']['class_means']\n",
    "    qni_ccp.class_counts = checkpoint['qni_ccp_state']['class_counts']\n",
    "    qni_ccp.current_epsilon = checkpoint['qni_ccp_state']['current_epsilon']\n",
    "    \n",
    "    print(f\"✅ Best model loaded from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"✅ Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "    print(f\"✅ Final QNI-CCP ε: {qni_ccp.current_epsilon:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️  No saved model found, using current model state\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss, test_acc, test_robust_acc = evaluate_with_qni_ccp(\n",
    "    model, test_loader, loss_fn, device, qni_ccp, test_robustness=True\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 FINAL TEST RESULTS:\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   Test Robust Accuracy: {test_robust_acc:.4f}\")\n",
    "print(f\"   Robustness Drop: {(test_acc - test_robust_acc):.4f}\")\n",
    "\n",
    "# %%\n",
    "# Comprehensive robustness analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🛡️  COMPREHENSIVE ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "original_epsilon = qni_ccp.current_epsilon\n",
    "epsilons = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "robustness_results = []\n",
    "\n",
    "print(\"Testing robustness across different perturbation strengths...\")\n",
    "for eps in epsilons:\n",
    "    qni_ccp.current_epsilon = eps\n",
    "    _, _, robust_acc = evaluate_with_qni_ccp(model, test_loader, loss_fn, \n",
    "                                           device, qni_ccp, test_robustness=True)\n",
    "    robustness_results.append(robust_acc)\n",
    "    print(f\"   ε = {eps:.2f}: Robust Accuracy = {robust_acc:.4f}\")\n",
    "\n",
    "# Restore original epsilon\n",
    "qni_ccp.current_epsilon = original_epsilon\n",
    "\n",
    "# Plot robustness analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epsilons, robustness_results, 'b-o', linewidth=3, markersize=8)\n",
    "plt.axhline(y=test_acc, color='red', linestyle='--', alpha=0.7, label=f'Clean Accuracy ({test_acc:.3f})')\n",
    "plt.xlabel('Perturbation Strength (ε)')\n",
    "plt.ylabel('Robust Accuracy')\n",
    "plt.title('QNI-CCP Robustness vs Perturbation Strength')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Robustness drop analysis\n",
    "robustness_drop = [test_acc - acc for acc in robustness_results]\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epsilons, robustness_drop, 'r-s', linewidth=3, markersize=8)\n",
    "plt.xlabel('Perturbation Strength (ε)')\n",
    "plt.ylabel('Accuracy Drop')\n",
    "plt.title('Robustness Degradation Analysis')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Training summary\n",
    "plt.subplot(2, 2, 3)\n",
    "final_epochs = len(train_losses)\n",
    "x_epochs = range(1, final_epochs + 1)\n",
    "plt.plot(x_epochs, train_accs, label='Train', linewidth=2)\n",
    "plt.plot(x_epochs, val_accs, label='Validation', linewidth=2)\n",
    "plt.plot(x_epochs, robust_accs, label='Robust', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Progress Summary')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final metrics comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "metrics = ['Train Acc', 'Val Acc', 'Test Acc', 'Robust Acc']\n",
    "values = [train_accs[-1], val_accs[-1], test_acc, test_robust_acc]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "bars = plt.bar(metrics, values, color=colors, alpha=0.7)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Final Performance Metrics')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 TRAINING SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Configuration Used:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nTraining Results:\")\n",
    "print(f\"  Total epochs completed: {len(train_losses)}\")\n",
    "print(f\"  Final train accuracy: {train_accs[-1]:.4f}\")\n",
    "print(f\"  Final validation accuracy: {val_accs[-1]:.4f}\")\n",
    "print(f\"  Final overfitting gap: {(train_accs[-1] - val_accs[-1]):.4f}\")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Test robust accuracy: {test_robust_acc:.4f}\")\n",
    "print(f\"  Robustness preservation: {(test_robust_acc/test_acc*100):.1f}%\")\n",
    "\n",
    "print(f\"\\nQNI-CCP Effectiveness:\")\n",
    "print(f\"  Classes tracked: {len(qni_ccp.class_means)}\")\n",
    "print(f\"  Final epsilon: {qni_ccp.current_epsilon:.4f}\")\n",
    "print(f\"  Quantum complexity factor: {qni_ccp.quantum_complexity:.4f}\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "best_val_idx = np.argmin(val_losses)\n",
    "print(f\"  Best validation loss: {val_losses[best_val_idx]:.4f} (epoch {best_val_idx+1})\")\n",
    "print(f\"  Best validation accuracy: {val_accs[best_val_idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 ENHANCED QNI-CCP TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0472c-ff07-40ba-a0b1-47aaa2a3ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np \n",
    "import random \n",
    "import os \n",
    "from torchvision.datasets import ImageFolder\n",
    "import pennylane as qml \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm.notebook import tqdm \n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# %%\n",
    "class Focal_loss(nn.Module):\n",
    "    def __init__(self, alpha, gamma, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        CE = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-CE)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * CE\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "# %%\n",
    "class QNI_CCP:\n",
    "    \"\"\"\n",
    "    Quantum-aware Noise Injection with Class-Conditional Perturbation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits, n_layers, num_classes, epsilon_base=0.1, \n",
    "                 update_freq=100, alpha=0.9):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon_base = epsilon_base\n",
    "        self.update_freq = update_freq\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Class-conditional statistics\n",
    "        self.class_means = {}\n",
    "        self.class_counts = defaultdict(int)\n",
    "        self.update_counter = 0\n",
    "        \n",
    "        # Quantum complexity metrics\n",
    "        self.circuit_depth = n_layers\n",
    "        self.entangling_gates = (n_qubits - 1) * n_layers  # CNOT gates\n",
    "        self.quantum_complexity = self._compute_quantum_complexity()\n",
    "        \n",
    "    def _compute_quantum_complexity(self):\n",
    "        \"\"\"Compute quantum-aware scaling factor based on circuit properties\"\"\"\n",
    "        # Normalize complexity metrics\n",
    "        depth_factor = min(self.circuit_depth / 10.0, 1.0)\n",
    "        entanglement_factor = min(self.entangling_gates / (self.n_qubits * 5), 1.0)\n",
    "        \n",
    "        # Higher complexity -> smaller perturbations\n",
    "        complexity = 0.5 * depth_factor + 0.5 * entanglement_factor\n",
    "        epsilon_q = 1.0 - complexity  # Inverse relationship\n",
    "        return max(epsilon_q, 0.1)  # Minimum threshold\n",
    "    \n",
    "    def update_class_statistics(self, features, labels):\n",
    "        \"\"\"Update running class means using exponential moving average\"\"\"\n",
    "        features = features.detach().cpu()\n",
    "        labels = labels.detach().cpu()\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = label.item()\n",
    "            feature_vec = features[i]\n",
    "            \n",
    "            if label_int not in self.class_means:\n",
    "                self.class_means[label_int] = feature_vec.clone()\n",
    "            else:\n",
    "                # Exponential moving average\n",
    "                self.class_means[label_int] = (self.alpha * self.class_means[label_int] + \n",
    "                                             (1 - self.alpha) * feature_vec)\n",
    "            \n",
    "            self.class_counts[label_int] += 1\n",
    "        \n",
    "        self.update_counter += 1\n",
    "    \n",
    "    def compute_gradient_sensitivity(self, model, features, labels, loss_fn):\n",
    "        \"\"\"\n",
    "        Compute gradient sensitivity for feature-level perturbations.\n",
    "        Sensitivity is defined as the absolute gradient of the loss w.r.t. extracted features.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "    \n",
    "        # Extract features (no grad needed here)\n",
    "        with torch.no_grad():\n",
    "            extracted_features = model.featureextractor(features)\n",
    "    \n",
    "        # Enable gradient tracking for extracted features\n",
    "        extracted_features.requires_grad_(True)\n",
    "    \n",
    "        # Forward pass through quantum layer and classifier\n",
    "        q_outputs = []\n",
    "        for i in range(extracted_features.size(0)):\n",
    "            q_out = model.q_layer(torch.tanh(extracted_features[i]))\n",
    "            q_outputs.append(q_out)\n",
    "        q_out_batch = torch.stack(q_outputs)\n",
    "    \n",
    "        # Classification output\n",
    "        outputs = model.classifier(q_out_batch)\n",
    "    \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "    \n",
    "        # Compute gradients w.r.t. extracted features\n",
    "        gradients = torch.autograd.grad(loss, extracted_features,\n",
    "                                        retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "        # Sensitivity = absolute value of gradient\n",
    "        sensitivity = torch.abs(gradients).detach()\n",
    "    \n",
    "        return sensitivity\n",
    "\n",
    "    def generate_perturbation(self, features, labels, model, loss_fn, training=True):\n",
    "        \"\"\"\n",
    "        Generate QNI-CCP perturbations on pre-tanh features using class-conditional shifts.\n",
    "        \"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "    \n",
    "        # Get raw features (pre-tanh)\n",
    "        with torch.no_grad():\n",
    "            extracted_features = model.featureextractor(features)\n",
    "    \n",
    "        # Skip if not enough class statistics\n",
    "        if len(self.class_means) < 2:\n",
    "            return torch.tanh(extracted_features)\n",
    "    \n",
    "        # Compute gradient sensitivity (still using pre-tanh features)\n",
    "        sensitivity = self.compute_gradient_sensitivity(model, features, labels, loss_fn)\n",
    "    \n",
    "        perturbed_features = []\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            current_label = labels[i].item()\n",
    "            current_feature = extracted_features[i]\n",
    "            current_sensitivity = sensitivity[i]\n",
    "    \n",
    "            # Skip if insufficient samples for this class\n",
    "            available_classes = [c for c in self.class_means\n",
    "                                 if c != current_label and self.class_counts[c] > 10]\n",
    "            if not available_classes:\n",
    "                perturbed_features.append(torch.tanh(current_feature))\n",
    "                continue\n",
    "    \n",
    "            # Pick a target class randomly\n",
    "            target_class = random.choice(available_classes)\n",
    "            target_mean = self.class_means[target_class].to(device)\n",
    "    \n",
    "            # Direction and scaling\n",
    "            direction = target_mean - current_feature\n",
    "            epsilon_q = self.quantum_complexity\n",
    "            scaled_perturbation = self.epsilon_base * epsilon_q * current_sensitivity * direction\n",
    "    \n",
    "            perturbed_feature = current_feature + scaled_perturbation\n",
    "    \n",
    "            # Push through tanh only at the end (constrain to quantum input space)\n",
    "            perturbed_features.append(torch.tanh(perturbed_feature))\n",
    "    \n",
    "        return torch.stack(perturbed_features)\n",
    "    \n",
    "    def apply_qni_ccp(self, model, features, labels, loss_fn, training=True, \n",
    "                     mix_ratio=0.5):\n",
    "        \"\"\"Apply QNI-CCP with clean/perturbed mixing\"\"\"\n",
    "        # Always get clean features and outputs first\n",
    "        clean_features = model.featureextractor(features)\n",
    "        clean_features = torch.tanh(clean_features)\n",
    "        clean_q_outputs = []\n",
    "        for i in range(clean_features.size(0)):\n",
    "            q_out = model.q_layer(clean_features[i])\n",
    "            clean_q_outputs.append(q_out)\n",
    "        clean_q_out_batch = torch.stack(clean_q_outputs)\n",
    "        clean_outputs = model.classifier(clean_q_out_batch)\n",
    "        \n",
    "        # Update class statistics during training (always do this)\n",
    "        if training:\n",
    "            self.update_class_statistics(clean_features, labels)\n",
    "        \n",
    "        # Apply QNI-CCP if we have enough class statistics and training\n",
    "        if training and len(self.class_means) >= 2 and random.random() < mix_ratio:\n",
    "            # Generate perturbed features\n",
    "            perturbed_features = self.generate_perturbation(features, labels, \n",
    "                                                          model, loss_fn, training)\n",
    "            \n",
    "            # Process through quantum layer and classifier\n",
    "            perturbed_q_outputs = []\n",
    "            for i in range(perturbed_features.size(0)):\n",
    "                q_out = model.q_layer(perturbed_features[i])\n",
    "                perturbed_q_outputs.append(q_out)\n",
    "            perturbed_q_out_batch = torch.stack(perturbed_q_outputs)\n",
    "            perturbed_outputs = model.classifier(perturbed_q_out_batch)\n",
    "            \n",
    "            # Mixed loss\n",
    "            clean_loss = loss_fn(clean_outputs, labels)\n",
    "            perturbed_loss = loss_fn(perturbed_outputs, labels)\n",
    "            total_loss = 0.8 * clean_loss + 0.2 * perturbed_loss\n",
    "            \n",
    "            return total_loss, clean_outputs\n",
    "        else:\n",
    "            # Standard forward pass\n",
    "            loss = loss_fn(clean_outputs, labels)\n",
    "            return loss, clean_outputs\n",
    "\n",
    "# %%\n",
    "# Set seeds for reproducibility\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# %%\n",
    "n_qubits = 6\n",
    "n_layers = 2\n",
    "num_classes = 25\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "lr = 0.0005\n",
    "\n",
    "# %%\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# %%\n",
    "# Update paths as needed\n",
    "train_dataset = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/train', transform=train_transform)\n",
    "test_dataset = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/test', transform=test_transform)\n",
    "val_dataset = ImageFolder('/home/netsec1/dataset_folder/malimg_dataset/val', transform=test_transform)\n",
    "\n",
    "# %%\n",
    "labels = [label for _, label in train_dataset.samples]\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "loss_fn = Focal_loss(alpha=1, gamma=2)\n",
    "\n",
    "# %%\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8,pin_memory=True)\n",
    "\n",
    "# %%\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits,shots=None)\n",
    "\n",
    "# %%\n",
    "# @qml.qnode(dev, interface=\"torch\")\n",
    "# def circuit(inputs, weights):\n",
    "#     # Encoding\n",
    "#     for i in range(n_qubits):\n",
    "#         qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "#     # Variational layers\n",
    "#     for l in range(weights.shape[0]):\n",
    "#         for i in range(n_qubits):\n",
    "#             qml.RY(weights[l][i], wires=i)\n",
    "        \n",
    "#         for i in range(n_qubits - 1):\n",
    "#             qml.CNOT(wires=[i, i + 1])\n",
    "    \n",
    "#     return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def circuit(inputs, weights):\n",
    "    # Initial encoding\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "    # Variational layers with improved entanglement\n",
    "    for l in range(weights.shape[0]):\n",
    "        # Parameterized rotations\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(weights[l][i][0], wires=i)\n",
    "            qml.RZ(weights[l][i][1], wires=i)\n",
    "        \n",
    "        # Circular entanglement for better connectivity\n",
    "        for i in range(n_qubits):\n",
    "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "        \n",
    "        # Data re-encoding (every other layer)\n",
    "        if l % 2 == 1:\n",
    "            for i in range(n_qubits):\n",
    "                qml.RY(inputs[i] * 0.1, wires=i)\n",
    "    \n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n",
    "\n",
    "# %%\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits,2)}\n",
    "\n",
    "# %%\n",
    "class FeatureReduce(nn.Module):\n",
    "    def __init__(self, final_dim, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),    # 128 -> 64\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),   # 64 -> 32\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 32 -> 16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 16 -> 8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # 8 -> 4\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                # 4×4 -> 1×1\n",
    "        )\n",
    "        self.fc = nn.Linear(128, final_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# %%\n",
    "class HybridQnn(nn.Module):\n",
    "    def __init__(self, n_qubits, num_classes):\n",
    "        super().__init__()\n",
    "        self.featureextractor = FeatureReduce(final_dim=n_qubits)\n",
    "        self.q_layer = qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)\n",
    "        # 4-layer MLP after quantum layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_qubits, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.featureextractor(x)\n",
    "        x = torch.tanh(x)\n",
    "        # Process each sample individually through quantum layer\n",
    "        q_outputs = []\n",
    "        for i in range(x.size(0)):\n",
    "            q_out = self.q_layer(x[i])\n",
    "            q_outputs.append(q_out)\n",
    "        q_out_batch = torch.stack(q_outputs)\n",
    "        return self.classifier(q_out_batch)\n",
    "\n",
    "# %%\n",
    "model = HybridQnn(n_qubits=n_qubits, num_classes=num_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "# Initialize QNI-CCP\n",
    "qni_ccp = QNI_CCP(n_qubits=n_qubits, n_layers=n_layers, num_classes=num_classes,\n",
    "                  epsilon_base=0.1, update_freq=100, alpha=0.9)\n",
    "\n",
    "# %%\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode=\"min\", factor=0.5, patience=5\n",
    "# )\n",
    "\n",
    "# Add warmup scheduler:\n",
    "def get_lr_scheduler(optimizer, warmup_epochs=5):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        return 1.0\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Use it:\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_epochs=5)\n",
    "\n",
    "# %%\n",
    "def train_with_qni_ccp(model, data_loader, loss_fn, optimizer, device, qni_ccp):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(data_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply QNI-CCP during training\n",
    "        loss, outputs = qni_ccp.apply_qni_ccp(model, inputs, labels, loss_fn, \n",
    "                                             training=True, mix_ratio=0.5)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        mix_ratio = min(0.2 + 0.02 * epoch, 0.5)\n",
    "\n",
    "    \n",
    "    return total_loss / len(data_loader), correct / len(data_loader.dataset)\n",
    "\n",
    "# %%\n",
    "def evaluate_with_qni_ccp(model, dataloader, loss_fn, device, qni_ccp, test_robustness=False):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    robust_correct = 0  # For robustness testing\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Standard evaluation\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Robustness testing with QNI-CCP perturbations\n",
    "            if test_robustness:\n",
    "                # Apply QNI-CCP perturbations\n",
    "                _, perturbed_outputs = qni_ccp.apply_qni_ccp(model, inputs, labels, \n",
    "                                                           loss_fn, training=False, \n",
    "                                                           mix_ratio=1.0)\n",
    "                _, robust_predicted = torch.max(perturbed_outputs.data, 1)\n",
    "                robust_correct += (robust_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    robust_accuracy = robust_correct / total if test_robustness else None\n",
    "    \n",
    "    return total_loss / len(dataloader), accuracy, robust_accuracy\n",
    "\n",
    "# %%\n",
    "# Training loop with QNI-CCP\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "robust_accs = []\n",
    "\n",
    "# Early Stopping variables\n",
    "early_stopping_patience = 6\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(\"Starting training with QNI-CCP...\")\n",
    "print(f\"Quantum complexity factor: {qni_ccp.quantum_complexity:.3f}\")\n",
    "print(f\"Dataset info:\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "\n",
    "# %%\n",
    "for epoch in range(num_epochs):\n",
    "    # Training with QNI-CCP\n",
    "    train_loss, train_acc = train_with_qni_ccp(model, train_loader, loss_fn, \n",
    "                                              optimizer, device, qni_ccp)\n",
    "    \n",
    "    # Validation (standard and robustness testing)\n",
    "    val_loss, val_acc, robust_acc = evaluate_with_qni_ccp(model, val_loader, loss_fn, \n",
    "                                                          device, qni_ccp, \n",
    "                                                          test_robustness=True)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    robust_accs.append(robust_acc)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"Robust Acc: {robust_acc:.4f}\")\n",
    "    print(f\"Class means tracked: {len(qni_ccp.class_means)}\")\n",
    "    print(f\"QNI-CCP active: {'Yes' if len(qni_ccp.class_means) >= 2 else 'No'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Visualization\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(train_accs, label='Train Acc')\n",
    "        plt.plot(val_accs, label='Val Acc')\n",
    "        plt.title('Accuracy Progress')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(val_accs, label='Standard Acc')\n",
    "        plt.plot(robust_accs, label='Robust Acc')\n",
    "        plt.title('Robustness Analysis')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Early Stopping Logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'qni_ccp_state': {\n",
    "                'class_means': qni_ccp.class_means,\n",
    "                'class_counts': qni_ccp.class_counts,\n",
    "                'quantum_complexity': qni_ccp.quantum_complexity\n",
    "            }\n",
    "        }, \"best_model_qni_ccp.pth\")\n",
    "        print(\"💾 Best model with QNI-CCP saved.\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"🕒 No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "    \n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(f\"⏹️ Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "\n",
    "# %%\n",
    "# Final evaluation on test set\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"best_model_qni_ccp.pth\", weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Restore QNI-CCP state\n",
    "qni_ccp.class_means = checkpoint['qni_ccp_state']['class_means']\n",
    "qni_ccp.class_counts = checkpoint['qni_ccp_state']['class_counts']\n",
    "\n",
    "# Test evaluation\n",
    "test_loss, test_acc, test_robust_acc = evaluate_with_qni_ccp(model, test_loader, loss_fn, \n",
    "                                                           device, qni_ccp, \n",
    "                                                           test_robustness=True)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Robust Accuracy: {test_robust_acc:.4f}\")\n",
    "print(f\"Robustness Drop: {(test_acc - test_robust_acc):.4f}\")\n",
    "\n",
    "# %%\n",
    "# Robustness analysis with different perturbation strengths\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ROBUSTNESS ANALYSIS WITH DIFFERENT PERTURBATION STRENGTHS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "epsilons = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "robustness_results = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    qni_ccp.epsilon_base = eps\n",
    "    _, _, robust_acc = evaluate_with_qni_ccp(model, test_loader, loss_fn, \n",
    "                                           device, qni_ccp, test_robustness=True)\n",
    "    robustness_results.append(robust_acc)\n",
    "    print(f\"Epsilon: {eps:.2f} | Robust Accuracy: {robust_acc:.4f}\")\n",
    "\n",
    "# Plot robustness vs perturbation strength\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, robustness_results, 'b-o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Perturbation Strength (ε)')\n",
    "plt.ylabel('Robust Accuracy')\n",
    "plt.title('QNI-CCP Robustness Analysis')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nQNI-CCP Integration Complete!\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Final Robust Accuracy: {test_robust_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
